#!/bin/bash

if [ $# -lt 6  ]; then
    echo "<user> <op> <proj> <arg> <offline> <hadoop_home>"
    echo "incorrect usage."
    echo "<arg> is the python version for 'create' and the source env for 'clone'."
 exit 1
fi

EXEC_AS_USER=$1
OP=$2
PROJECT=$3
PYTHON_VERSION=$4
OFFLINE=$5
HADOOP_DIR=$6

CLONE=
CONDA_DIR=<%= node["conda"]["base_dir"] %>
WITH_PYTHON_KERNEL=<%= @jupyter_python %>

if [ "$OP" == "CLONE" ] ; then
   CLONE="--clone $4"
fi

function exists() {
    su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda info --envs | grep '^${PROJECT} '"
    return $?
}

if [ "$OP" == "CREATE" ] ; then
    exists
    if [ $? -eq 0 ] ; then 
       exit 0
    fi

    if [[ $PYTHON_VERSION =~ .*X ]] ; then
       PYTHON_VERSION=${PYTHON_VERSION::-1} # removes the 'X' at the end of the version
       WITH_PYTHON_KERNEL="false"
    fi
    su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda create -n $PROJECT python=$PYTHON_VERSION -y -q $OFFLINE"

    SYMBOLIC_LINK_HADOOP="$(dirname "$HADOOP_DIR")/hadoop"

    if [ -L $SYMBOLIC_LINK_HADOOP ]; then
       HADOOP_DIR=$SYMBOLIC_LINK_HADOOP
    fi

    echo "Hadoop dir is $HADOOP_DIR"
    #export HADOOP_CONF_DIR=${HADOOP_DIR}/hadoop/etc
    export HADOOP_HOME=$HADOOP_DIR

    parentdir="$(dirname "$dir")"

    export PY=$(echo $PYTHON_VERSION | sed 's/\.//g')

    if [ "$PY" == "36" ] ; then
        su $EXEC_AS_USER -c "export HADOOP_HOME=$HADOOP_DIR && ${CONDA_DIR}/envs/${PROJECT}/bin/pip install git+https://github.com/crs4/pydoop.git@develop"
    else  # python 2.7
	    su $EXEC_AS_USER -c "export HADOOP_HOME=$HADOOP_DIR && ${CONDA_DIR}/envs/${PROJECT}/bin/pip install --upgrade pydoop"
	    # Tensorflow serving only works with Python2
        su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install --upgrade tensorflow-serving-api"
    fi

    su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install --upgrade hopsfacets"
    su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install --upgrade tfspark"
    if [ "$WITH_PYTHON_KERNEL" == "true" ] ; then    
      su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install ipykernel"
      #su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/conda install pandas"
      #su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/conda install numpy"
    fi

    # Install a custom build of tensorflow with this line.
    #    su $EXEC_AS_USER -c "<%= node['conda']['base_dir'] %>/envs/${PROJECT}/bin/pip install --upgrade <%= node['conda']['base_dir'] %>/pkgs/tensorflow${GPU}-<%= node['tensorflow']['version'] %>-cp${PY}-cp${PY}mu-manylinux1_x86_64.whl"
    # 'pip install tensorflow' include HDFS support. 'conda install tensorflow-gpu' does not!

    # If cuda is installed, and there is a GPU, install TF with GPUs
    GPU=
    if [ -f /usr/local/cuda/version.txt ]  ; then
        nvidia-smi -L | grep -i gpu
    	if [ $? -eq 0 ] ; then
            GPU="-gpu"
            su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip uninstall tensorflow"
    	    if [ $? -ne 0 ] ; then
                echo "Problem uninstalling tensorflow to prepare for gpu version"
            fi
        fi
    fi

    su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install tensorflow${GPU}==<%= node['tensorflow']['version'] %>  --upgrade --force-reinstall"

    su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install horovod"

    su $EXEC_AS_USER -c "${CONDA_DIR}/envs/${PROJECT}/bin/pip install --upgrade hops"
    
elif [ "$OP" == "LIST_ENVS" ] ; then
  su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda env list --json > /tmp/conda_envs.json"    

elif [ "$OP" == "LIST" ] ; then
  su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda list -n $PROJECT --json > /tmp/${PROJECT}__conda_list.json"

elif [ "$OP" == "CLONE" ] ; then
  su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda create -n $PROJECT --clone $CLONE -y -q $OFFLINE"

elif [ "$OP" == "LIB_SYNC" ] ; then

    su <%= node['conda']['user'] %> -c "<%= node['conda']['base_dir'] %>/bin/conda list -n $1 --json > /tmp/${1}.json"
    if [ $? -ne 0 ] ; then
	echo "Problem listing conda libraries for project $1"
	exit 2
    fi
    if [ ! -f /tmp/${1}.json ] ; then
	echo "Problem finding fil with list of conda libraries for project $1"
	exit 2
    fi
    arr=( $(jq -r '.[].dist_name' /tmp/${1}.json) )
    printf '%s ' "${arr[@]}" > /tmp/${1}.libs

    # kagent has to take the one big json object and save it locally as one json object per project.
    server_libs=( $(jq -r  '.[].library' /tmp/${1}_server.json) )

    # For all the libs installed in Hopsworks, check each one is installed in the local conda environment
    #for i in "${arr[@]}"
    for i in "${server_libs[@]}"
    do
	# If i cannot find the library installed locally, install it in this project
	grep $i /tmp/${1}.libs
	if [ $? -ne 0 ] ; then
	    su <%= node['conda']['user'] %> -c "<%= node['conda']['base_dir'] %>/bin/conda install -n $1 -q -y $2 $i"
	fi
    done
    
elif [ "$OP" == "REMOVE" ] ; then
    PROJECTUSERNAME=$PYTHON_VERSION
    exists    # If the env has already been removed, return OK
    if [ $? -ne 0 ] ; then
       echo "Cannot remove environment that doesn't exist"
       exit 0
    fi
    su $EXEC_AS_USER -c "${CONDA_DIR}/bin/conda-env remove -n $PROJECT -y -q"
    rm -rf ${CONDA_DIR}/envs/${PROJECT}
    if [ "${WITH_PYTHON_KERNEL}" == "true" ] ; then
	kernelsToDelete=$(sudo jupyter-kernelspec list | grep ${PROJECT}__ | sed -e 's/.*\/usr\/local\/share\/jupyter\/kernels\///g')
        echo "Kernels to delete:"
        echo $kernelsToDelete
        if [ "$kernelsToDelete" != "" ] ; then
	  while read -r kernel; do
             echo "sudo jupyter-kernelspec remove -y -f ${kernel}"
             sudo jupyter-kernelspec remove -y -f ${kernel} || 0
	  done <<< "$kernelsToDelete"
        fi
    fi

else
    exit -1
fi
